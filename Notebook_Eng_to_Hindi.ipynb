{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Importing libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom collections import defaultdict\nimport string\nimport tensorflow as tf\nimport re\nimport os\nimport time\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:36.539276Z","iopub.execute_input":"2022-11-24T18:18:36.539583Z","iopub.status.idle":"2022-11-24T18:18:42.259094Z","shell.execute_reply.started":"2022-11-24T18:18:36.539549Z","shell.execute_reply":"2022-11-24T18:18:42.258367Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2022-11-24 18:18:37.869604: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","output_type":"stream"}]},{"cell_type":"code","source":"#Declaring some required variables\nENCODER_LEN = 100\nDECODER_LEN = 100\nBATCH_SIZE = 128\nBUFFER_SIZE = BATCH_SIZE*4\nos.listdir('/kaggle/input/')","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:42.260844Z","iopub.execute_input":"2022-11-24T18:18:42.261223Z","iopub.status.idle":"2022-11-24T18:18:42.269959Z","shell.execute_reply.started":"2022-11-24T18:18:42.261188Z","shell.execute_reply":"2022-11-24T18:18:42.269122Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['hindi-english-truncated-corpus']"},"metadata":{}}]},{"cell_type":"code","source":"#DataSet is taken from Kaggle\n\ntrain_df=pd.read_csv(\"/kaggle/input/hindi-english-truncated-corpus/Hindi_English_Truncated_Corpus.csv\",encoding='utf-8')\ntrain_df.drop(['source'],axis=1,inplace=True)\nmask = (train_df['english_sentence'].str.len()>20) & (train_df['english_sentence'].str.len()<200)\ntrain_df = train_df.loc[mask]\ntrain_df = train_df.sample(64000, random_state=1)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:42.271496Z","iopub.execute_input":"2022-11-24T18:18:42.271800Z","iopub.status.idle":"2022-11-24T18:18:43.720798Z","shell.execute_reply.started":"2022-11-24T18:18:42.271764Z","shell.execute_reply":"2022-11-24T18:18:43.719711Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                        english_sentence  \\\n63241         Indian News Service - National News Agency   \n81404  In West Bengal , it seems set to eat humble pi...   \n8803   One american dollar is equal to 60 pakistani r...   \n73434                      but between those high highs,   \n65711  Every other politician went along because when...   \n\n                                          hindi_sentence  \n63241     इण्डियन न्यूज सर्विस - राष्ट्रीय समाचार एजेंसी  \n81404  पश्चिम बंगाल में तो वह अपमान का घूंट पीने को भ...  \n8803   एक अमरीकी डालर की कीमत लगभग ६० पाकिस्तानी रुपय...  \n73434                     लेकिन इन बेहतरीन लम्हों के बीच  \n65711  और वजह यह थी कि आर्थिक मामलं पर हमेशा विफल विच...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>english_sentence</th>\n      <th>hindi_sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>63241</th>\n      <td>Indian News Service - National News Agency</td>\n      <td>इण्डियन न्यूज सर्विस - राष्ट्रीय समाचार एजेंसी</td>\n    </tr>\n    <tr>\n      <th>81404</th>\n      <td>In West Bengal , it seems set to eat humble pi...</td>\n      <td>पश्चिम बंगाल में तो वह अपमान का घूंट पीने को भ...</td>\n    </tr>\n    <tr>\n      <th>8803</th>\n      <td>One american dollar is equal to 60 pakistani r...</td>\n      <td>एक अमरीकी डालर की कीमत लगभग ६० पाकिस्तानी रुपय...</td>\n    </tr>\n    <tr>\n      <th>73434</th>\n      <td>but between those high highs,</td>\n      <td>लेकिन इन बेहतरीन लम्हों के बीच</td>\n    </tr>\n    <tr>\n      <th>65711</th>\n      <td>Every other politician went along because when...</td>\n      <td>और वजह यह थी कि आर्थिक मामलं पर हमेशा विफल विच...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:43.725459Z","iopub.execute_input":"2022-11-24T18:18:43.727552Z","iopub.status.idle":"2022-11-24T18:18:43.733152Z","shell.execute_reply.started":"2022-11-24T18:18:43.727501Z","shell.execute_reply":"2022-11-24T18:18:43.732475Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#Adding <SOS> and <END> tokens \n\neng = train_df['english_sentence']\nhind = train_df['hindi_sentence']\neng = eng.apply(lambda x: \"<SOS> \" + str(x) + \" <EOS>\")\nhind = hind.apply(lambda x: \"<SOS> \"+ x + \" <EOS>\")","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:43.737355Z","iopub.execute_input":"2022-11-24T18:18:43.739005Z","iopub.status.idle":"2022-11-24T18:18:43.855480Z","shell.execute_reply.started":"2022-11-24T18:18:43.738829Z","shell.execute_reply":"2022-11-24T18:18:43.854743Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Text Preprocessing for Model\n\nfilters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\noov_token = '<unk>'\neng_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = filters, oov_token=oov_token)\nhind_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = filters, oov_token=oov_token)\neng_tokenizer.fit_on_texts(eng)\nhind_tokenizer.fit_on_texts(hind)\ninputs = eng_tokenizer.texts_to_sequences(eng)\ntargets = hind_tokenizer.texts_to_sequences(hind)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:43.859678Z","iopub.execute_input":"2022-11-24T18:18:43.861545Z","iopub.status.idle":"2022-11-24T18:18:49.591856Z","shell.execute_reply.started":"2022-11-24T18:18:43.861495Z","shell.execute_reply":"2022-11-24T18:18:49.591111Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Vocabulary size\n\nENCODER_VOCAB = len(eng_tokenizer.word_index) + 1\nDECODER_VOCAB = len(hind_tokenizer.word_index) + 1\nprint(ENCODER_VOCAB, DECODER_VOCAB)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:49.593201Z","iopub.execute_input":"2022-11-24T18:18:49.594038Z","iopub.status.idle":"2022-11-24T18:18:49.599732Z","shell.execute_reply.started":"2022-11-24T18:18:49.594002Z","shell.execute_reply":"2022-11-24T18:18:49.598990Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"44305 51960\n","output_type":"stream"}]},{"cell_type":"code","source":"#Padding the inputs\n\ninputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=ENCODER_LEN, padding='post', truncating='post')\ntargets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=DECODER_LEN, padding='post', truncating='post')\ninputs = tf.cast(inputs, dtype=tf.int64)\ntargets = tf.cast(targets, dtype=tf.int64)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:49.601244Z","iopub.execute_input":"2022-11-24T18:18:49.601802Z","iopub.status.idle":"2022-11-24T18:18:52.469457Z","shell.execute_reply.started":"2022-11-24T18:18:49.601751Z","shell.execute_reply":"2022-11-24T18:18:52.468666Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"2022-11-24 18:18:50.189513: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2022-11-24 18:18:50.193287: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n2022-11-24 18:18:50.257871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-24 18:18:50.258581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \npciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n2022-11-24 18:18:50.258665: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n2022-11-24 18:18:50.290137: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n2022-11-24 18:18:50.290238: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n2022-11-24 18:18:50.312189: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n2022-11-24 18:18:50.328955: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n2022-11-24 18:18:50.360111: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n2022-11-24 18:18:50.378585: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n2022-11-24 18:18:50.381705: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n2022-11-24 18:18:50.381877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-24 18:18:50.382733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-24 18:18:50.384386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n2022-11-24 18:18:50.386905: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-24 18:18:50.388093: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2022-11-24 18:18:50.388312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-24 18:18:50.388970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \npciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n2022-11-24 18:18:50.389045: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n2022-11-24 18:18:50.389072: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n2022-11-24 18:18:50.389098: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n2022-11-24 18:18:50.389186: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n2022-11-24 18:18:50.389223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n2022-11-24 18:18:50.389241: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n2022-11-24 18:18:50.389260: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n2022-11-24 18:18:50.389279: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n2022-11-24 18:18:50.389392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-24 18:18:50.390144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-24 18:18:50.390719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n2022-11-24 18:18:50.391738: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n2022-11-24 18:18:52.081652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n2022-11-24 18:18:52.081702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n2022-11-24 18:18:52.081714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n2022-11-24 18:18:52.083985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-24 18:18:52.084869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-24 18:18:52.085543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-24 18:18:52.086108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14957 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:52.470913Z","iopub.execute_input":"2022-11-24T18:18:52.471171Z","iopub.status.idle":"2022-11-24T18:18:52.570970Z","shell.execute_reply.started":"2022-11-24T18:18:52.471134Z","shell.execute_reply":"2022-11-24T18:18:52.570322Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#Code for Implementing Transformer Model\n\n\ndef get_angles(position, i, d_model):\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n    return position * angle_rates\n\ndef positional_encoding(position, d_model):\n    angle_rads = get_angles(\n        np.arange(position)[:, np.newaxis],\n        np.arange(d_model)[np.newaxis, :],\n        d_model\n    )\n\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n    pos_encoding = angle_rads[np.newaxis, ...]\n\n    return tf.cast(pos_encoding, dtype=tf.float32)\n\ndef create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n    return seq[:, tf.newaxis, tf.newaxis, :]\n\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask\n\ndef scaled_dot_product_attention(q, k, v, mask):\n    matmul_qk = tf.matmul(q, k, transpose_b=True)\n\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)  \n\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n\n    output = tf.matmul(attention_weights, v)\n    return output, attention_weights\n","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:52.573979Z","iopub.execute_input":"2022-11-24T18:18:52.574670Z","iopub.status.idle":"2022-11-24T18:18:52.585102Z","shell.execute_reply.started":"2022-11-24T18:18:52.574633Z","shell.execute_reply":"2022-11-24T18:18:52.584384Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model // self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n        \n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)\n        k = self.wk(k)\n        v = self.wv(v)\n\n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n\n        scaled_attention, attention_weights = scaled_dot_product_attention(\n            q, k, v, mask)\n\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n        \n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n        output = self.dense(concat_attention)\n            \n        return output, attention_weights\n    \ndef point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(dff, activation='relu'),\n        tf.keras.layers.Dense(d_model)\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:52.586426Z","iopub.execute_input":"2022-11-24T18:18:52.586707Z","iopub.status.idle":"2022-11-24T18:18:52.600182Z","shell.execute_reply.started":"2022-11-24T18:18:52.586671Z","shell.execute_reply":"2022-11-24T18:18:52.599550Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n    \n    def call(self, x, training, mask):\n        attn_output, _ = self.mha(x, x, x, mask)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)\n\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)\n\n        return out2","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:52.601368Z","iopub.execute_input":"2022-11-24T18:18:52.601987Z","iopub.status.idle":"2022-11-24T18:18:52.613328Z","shell.execute_reply.started":"2022-11-24T18:18:52.601944Z","shell.execute_reply":"2022-11-24T18:18:52.612556Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n    \n    \n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n\n        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)\n\n        ffn_output = self.ffn(out2)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)\n\n        return out3, attn_weights_block1, attn_weights_block2","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:52.614422Z","iopub.execute_input":"2022-11-24T18:18:52.615446Z","iopub.status.idle":"2022-11-24T18:18:52.628316Z","shell.execute_reply.started":"2022-11-24T18:18:52.615406Z","shell.execute_reply":"2022-11-24T18:18:52.627601Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n        super(Encoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n\n        self.dropout = tf.keras.layers.Dropout(rate)\n        \n    def call(self, x, training, mask):\n        seq_len = tf.shape(x)[1]\n\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n    \n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n    \n        return x\n    \nclass Decoder(tf.keras.layers.Layer):\n        \n    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n        super(Decoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n    \n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n\n            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n    \n        return x, attention_weights\n    ","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:52.629493Z","iopub.execute_input":"2022-11-24T18:18:52.629808Z","iopub.status.idle":"2022-11-24T18:18:52.645656Z","shell.execute_reply.started":"2022-11-24T18:18:52.629777Z","shell.execute_reply":"2022-11-24T18:18:52.644940Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n    \n    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n        enc_output = self.encoder(inp, training, enc_padding_mask)\n\n        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n\n        final_output = self.final_layer(dec_output)\n\n        return final_output, attention_weights","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:52.646703Z","iopub.execute_input":"2022-11-24T18:18:52.647054Z","iopub.status.idle":"2022-11-24T18:18:52.659675Z","shell.execute_reply.started":"2022-11-24T18:18:52.647021Z","shell.execute_reply":"2022-11-24T18:18:52.658916Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"num_layers = 4\nd_model = 128\ndff = 512\nnum_heads = 8\ndropout_rate = 0.1\nEPOCHS = 50","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:52.660736Z","iopub.execute_input":"2022-11-24T18:18:52.661089Z","iopub.status.idle":"2022-11-24T18:18:52.672585Z","shell.execute_reply.started":"2022-11-24T18:18:52.661058Z","shell.execute_reply":"2022-11-24T18:18:52.671866Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n\n        self.warmup_steps = warmup_steps\n    \n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:52.674103Z","iopub.execute_input":"2022-11-24T18:18:52.674411Z","iopub.status.idle":"2022-11-24T18:18:52.683068Z","shell.execute_reply.started":"2022-11-24T18:18:52.674380Z","shell.execute_reply":"2022-11-24T18:18:52.682168Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"learning_rate = CustomSchedule(d_model)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:52.684331Z","iopub.execute_input":"2022-11-24T18:18:52.685239Z","iopub.status.idle":"2022-11-24T18:18:52.693915Z","shell.execute_reply.started":"2022-11-24T18:18:52.685211Z","shell.execute_reply":"2022-11-24T18:18:52.693075Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"temp_learning_rate_schedule = CustomSchedule(d_model)\n\nplt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\nplt.ylabel(\"Learning Rate\")\nplt.xlabel(\"Train Step\")","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:52.695200Z","iopub.execute_input":"2022-11-24T18:18:52.695672Z","iopub.status.idle":"2022-11-24T18:18:52.982602Z","shell.execute_reply.started":"2022-11-24T18:18:52.695641Z","shell.execute_reply":"2022-11-24T18:18:52.981934Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"Text(0.5, 0, 'Train Step')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n\n\ndef accuracy_function(real, pred):\n    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    accuracies = tf.math.logical_and(mask, accuracies)\n\n    accuracies = tf.cast(accuracies, dtype=tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:52.983963Z","iopub.execute_input":"2022-11-24T18:18:52.984363Z","iopub.status.idle":"2022-11-24T18:18:52.992099Z","shell.execute_reply.started":"2022-11-24T18:18:52.984327Z","shell.execute_reply":"2022-11-24T18:18:52.991323Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.Mean(name='train_accuracy')","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:52.993472Z","iopub.execute_input":"2022-11-24T18:18:52.993774Z","iopub.status.idle":"2022-11-24T18:18:53.022485Z","shell.execute_reply.started":"2022-11-24T18:18:52.993736Z","shell.execute_reply":"2022-11-24T18:18:53.021891Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"#Defining the Transformer based model\n\ntransformer = Transformer(\n    num_layers=num_layers,\n    d_model=d_model,\n    num_heads=num_heads,\n    dff=dff,\n    input_vocab_size=ENCODER_VOCAB,\n    target_vocab_size=DECODER_VOCAB,\n    pe_input=1000,\n    pe_target=1000,\n    rate=dropout_rate)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:53.023878Z","iopub.execute_input":"2022-11-24T18:18:53.024137Z","iopub.status.idle":"2022-11-24T18:18:53.134028Z","shell.execute_reply.started":"2022-11-24T18:18:53.024105Z","shell.execute_reply":"2022-11-24T18:18:53.133279Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def create_masks(inp, tar):\n    enc_padding_mask = create_padding_mask(inp)\n    dec_padding_mask = create_padding_mask(inp)\n\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n  \n    return enc_padding_mask, combined_mask, dec_padding_mask","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:53.135501Z","iopub.execute_input":"2022-11-24T18:18:53.135971Z","iopub.status.idle":"2022-11-24T18:18:53.141214Z","shell.execute_reply.started":"2022-11-24T18:18:53.135938Z","shell.execute_reply":"2022-11-24T18:18:53.140474Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(inp, tar):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n\n    with tf.GradientTape() as tape:\n        predictions, _ = transformer(\n            inp, tar_inp, \n            True, \n            enc_padding_mask, \n            combined_mask, \n            dec_padding_mask\n        )\n        loss = loss_function(tar_real, predictions)\n\n    gradients = tape.gradient(loss, transformer.trainable_variables)    \n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n\n    train_loss(loss)\n    train_accuracy(accuracy_function(tar_real, predictions))","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:53.142564Z","iopub.execute_input":"2022-11-24T18:18:53.142874Z","iopub.status.idle":"2022-11-24T18:18:53.151629Z","shell.execute_reply.started":"2022-11-24T18:18:53.142838Z","shell.execute_reply":"2022-11-24T18:18:53.150946Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#training the model\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n\n    train_loss.reset_states()\n  \n    for (batch, (inp, tar)) in enumerate(dataset):\n        train_step(inp, tar)\n    \n        if batch % 200 == 0:\n            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n      \n    \n   \n    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))","metadata":{"execution":{"iopub.status.busy":"2022-11-24T18:18:53.152920Z","iopub.execute_input":"2022-11-24T18:18:53.153212Z","iopub.status.idle":"2022-11-24T21:08:53.724018Z","shell.execute_reply.started":"2022-11-24T18:18:53.153172Z","shell.execute_reply":"2022-11-24T21:08:53.723244Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"2022-11-24 18:19:00.578922: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n2022-11-24 18:19:00.625342: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000185000 Hz\n2022-11-24 18:19:01.856678: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n2022-11-24 18:19:02.760079: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Batch 0 Loss 10.8624 Accuracy 0.0000\nEpoch 1 Batch 200 Loss 10.5045 Accuracy 0.0461\nEpoch 1 Batch 400 Loss 9.6674 Accuracy 0.0534\nEpoch 1 Loss 9.2536 Accuracy 0.0548\nTime taken for 1 epoch: 213.77793836593628 secs\n\nEpoch 2 Batch 0 Loss 7.3922 Accuracy 0.0548\nEpoch 2 Batch 200 Loss 7.2223 Accuracy 0.0609\nEpoch 2 Batch 400 Loss 6.9997 Accuracy 0.0729\nEpoch 2 Loss 6.9069 Accuracy 0.0785\nTime taken for 1 epoch: 203.1153702735901 secs\n\nEpoch 3 Batch 0 Loss 6.4680 Accuracy 0.0786\nEpoch 3 Batch 200 Loss 6.3398 Accuracy 0.0895\nEpoch 3 Batch 400 Loss 6.2269 Accuracy 0.0993\nEpoch 3 Loss 6.1762 Accuracy 0.1037\nTime taken for 1 epoch: 203.43533205986023 secs\n\nEpoch 4 Batch 0 Loss 6.0061 Accuracy 0.1037\nEpoch 4 Batch 200 Loss 5.8345 Accuracy 0.1120\nEpoch 4 Batch 400 Loss 5.7439 Accuracy 0.1200\nEpoch 4 Loss 5.7014 Accuracy 0.1238\nTime taken for 1 epoch: 203.5463559627533 secs\n\nEpoch 5 Batch 0 Loss 5.4949 Accuracy 0.1238\nEpoch 5 Batch 200 Loss 5.3987 Accuracy 0.1311\nEpoch 5 Batch 400 Loss 5.3078 Accuracy 0.1384\nEpoch 5 Loss 5.2675 Accuracy 0.1419\nTime taken for 1 epoch: 203.39624857902527 secs\n\nEpoch 6 Batch 0 Loss 5.0056 Accuracy 0.1420\nEpoch 6 Batch 200 Loss 4.9723 Accuracy 0.1489\nEpoch 6 Batch 400 Loss 4.8832 Accuracy 0.1560\nEpoch 6 Loss 4.8438 Accuracy 0.1595\nTime taken for 1 epoch: 203.6595277786255 secs\n\nEpoch 7 Batch 0 Loss 4.6302 Accuracy 0.1595\nEpoch 7 Batch 200 Loss 4.5626 Accuracy 0.1664\nEpoch 7 Batch 400 Loss 4.4783 Accuracy 0.1734\nEpoch 7 Loss 4.4420 Accuracy 0.1769\nTime taken for 1 epoch: 203.59473752975464 secs\n\nEpoch 8 Batch 0 Loss 4.2451 Accuracy 0.1769\nEpoch 8 Batch 200 Loss 4.1746 Accuracy 0.1839\nEpoch 8 Batch 400 Loss 4.0961 Accuracy 0.1911\nEpoch 8 Loss 4.0638 Accuracy 0.1946\nTime taken for 1 epoch: 203.30273127555847 secs\n\nEpoch 9 Batch 0 Loss 3.8602 Accuracy 0.1946\nEpoch 9 Batch 200 Loss 3.8103 Accuracy 0.2018\nEpoch 9 Batch 400 Loss 3.7262 Accuracy 0.2091\nEpoch 9 Loss 3.6902 Accuracy 0.2128\nTime taken for 1 epoch: 203.471364736557 secs\n\nEpoch 10 Batch 0 Loss 3.6448 Accuracy 0.2128\nEpoch 10 Batch 200 Loss 3.4294 Accuracy 0.2202\nEpoch 10 Batch 400 Loss 3.3593 Accuracy 0.2278\nEpoch 10 Loss 3.3297 Accuracy 0.2314\nTime taken for 1 epoch: 203.58171319961548 secs\n\nEpoch 11 Batch 0 Loss 3.2480 Accuracy 0.2315\nEpoch 11 Batch 200 Loss 3.1168 Accuracy 0.2389\nEpoch 11 Batch 400 Loss 3.0567 Accuracy 0.2464\nEpoch 11 Loss 3.0334 Accuracy 0.2500\nTime taken for 1 epoch: 203.32991480827332 secs\n\nEpoch 12 Batch 0 Loss 2.9171 Accuracy 0.2500\nEpoch 12 Batch 200 Loss 2.8620 Accuracy 0.2574\nEpoch 12 Batch 400 Loss 2.8140 Accuracy 0.2646\nEpoch 12 Loss 2.7960 Accuracy 0.2681\nTime taken for 1 epoch: 203.21346282958984 secs\n\nEpoch 13 Batch 0 Loss 2.8184 Accuracy 0.2681\nEpoch 13 Batch 200 Loss 2.6581 Accuracy 0.2752\nEpoch 13 Batch 400 Loss 2.6194 Accuracy 0.2821\nEpoch 13 Loss 2.6035 Accuracy 0.2855\nTime taken for 1 epoch: 203.53355813026428 secs\n\nEpoch 14 Batch 0 Loss 2.6128 Accuracy 0.2855\nEpoch 14 Batch 200 Loss 2.4879 Accuracy 0.2922\nEpoch 14 Batch 400 Loss 2.4528 Accuracy 0.2988\nEpoch 14 Loss 2.4409 Accuracy 0.3020\nTime taken for 1 epoch: 203.56554412841797 secs\n\nEpoch 15 Batch 0 Loss 2.4471 Accuracy 0.3021\nEpoch 15 Batch 200 Loss 2.3421 Accuracy 0.3084\nEpoch 15 Batch 400 Loss 2.3128 Accuracy 0.3147\nEpoch 15 Loss 2.3031 Accuracy 0.3177\nTime taken for 1 epoch: 203.46685814857483 secs\n\nEpoch 16 Batch 0 Loss 2.4670 Accuracy 0.3177\nEpoch 16 Batch 200 Loss 2.2177 Accuracy 0.3238\nEpoch 16 Batch 400 Loss 2.1928 Accuracy 0.3297\nEpoch 16 Loss 2.1845 Accuracy 0.3326\nTime taken for 1 epoch: 203.29325485229492 secs\n\nEpoch 17 Batch 0 Loss 2.2038 Accuracy 0.3326\nEpoch 17 Batch 200 Loss 2.1110 Accuracy 0.3383\nEpoch 17 Batch 400 Loss 2.0902 Accuracy 0.3439\nEpoch 17 Loss 2.0813 Accuracy 0.3466\nTime taken for 1 epoch: 203.26180481910706 secs\n\nEpoch 18 Batch 0 Loss 2.1228 Accuracy 0.3467\nEpoch 18 Batch 200 Loss 2.0186 Accuracy 0.3521\nEpoch 18 Batch 400 Loss 1.9969 Accuracy 0.3574\nEpoch 18 Loss 1.9895 Accuracy 0.3600\nTime taken for 1 epoch: 203.22346425056458 secs\n\nEpoch 19 Batch 0 Loss 2.1318 Accuracy 0.3600\nEpoch 19 Batch 200 Loss 1.9279 Accuracy 0.3651\nEpoch 19 Batch 400 Loss 1.9090 Accuracy 0.3701\nEpoch 19 Loss 1.9029 Accuracy 0.3726\nTime taken for 1 epoch: 203.08043313026428 secs\n\nEpoch 20 Batch 0 Loss 1.9153 Accuracy 0.3726\nEpoch 20 Batch 200 Loss 1.8481 Accuracy 0.3774\nEpoch 20 Batch 400 Loss 1.8307 Accuracy 0.3822\nEpoch 20 Loss 1.8250 Accuracy 0.3845\nTime taken for 1 epoch: 203.54791235923767 secs\n\nEpoch 21 Batch 0 Loss 1.7041 Accuracy 0.3845\nEpoch 21 Batch 200 Loss 1.7719 Accuracy 0.3892\nEpoch 21 Batch 400 Loss 1.7538 Accuracy 0.3937\nEpoch 21 Loss 1.7497 Accuracy 0.3959\nTime taken for 1 epoch: 203.24726748466492 secs\n\nEpoch 22 Batch 0 Loss 1.6519 Accuracy 0.3960\nEpoch 22 Batch 200 Loss 1.7026 Accuracy 0.4004\nEpoch 22 Batch 400 Loss 1.6866 Accuracy 0.4047\nEpoch 22 Loss 1.6821 Accuracy 0.4068\nTime taken for 1 epoch: 203.0619022846222 secs\n\nEpoch 23 Batch 0 Loss 1.6532 Accuracy 0.4068\nEpoch 23 Batch 200 Loss 1.6337 Accuracy 0.4110\nEpoch 23 Batch 400 Loss 1.6204 Accuracy 0.4151\nEpoch 23 Loss 1.6167 Accuracy 0.4172\nTime taken for 1 epoch: 203.64144802093506 secs\n\nEpoch 24 Batch 0 Loss 1.4514 Accuracy 0.4172\nEpoch 24 Batch 200 Loss 1.5760 Accuracy 0.4212\nEpoch 24 Batch 400 Loss 1.5625 Accuracy 0.4251\nEpoch 24 Loss 1.5578 Accuracy 0.4271\nTime taken for 1 epoch: 203.0948588848114 secs\n\nEpoch 25 Batch 0 Loss 1.5615 Accuracy 0.4271\nEpoch 25 Batch 200 Loss 1.5189 Accuracy 0.4309\nEpoch 25 Batch 400 Loss 1.5047 Accuracy 0.4347\nEpoch 25 Loss 1.5030 Accuracy 0.4365\nTime taken for 1 epoch: 203.43249344825745 secs\n\nEpoch 26 Batch 0 Loss 1.5264 Accuracy 0.4366\nEpoch 26 Batch 200 Loss 1.4624 Accuracy 0.4403\nEpoch 26 Batch 400 Loss 1.4524 Accuracy 0.4439\nEpoch 26 Loss 1.4495 Accuracy 0.4457\nTime taken for 1 epoch: 202.99563598632812 secs\n\nEpoch 27 Batch 0 Loss 1.4917 Accuracy 0.4457\nEpoch 27 Batch 200 Loss 1.4174 Accuracy 0.4492\nEpoch 27 Batch 400 Loss 1.4067 Accuracy 0.4527\nEpoch 27 Loss 1.4040 Accuracy 0.4544\nTime taken for 1 epoch: 203.32136702537537 secs\n\nEpoch 28 Batch 0 Loss 1.3873 Accuracy 0.4544\nEpoch 28 Batch 200 Loss 1.3714 Accuracy 0.4578\nEpoch 28 Batch 400 Loss 1.3623 Accuracy 0.4612\nEpoch 28 Loss 1.3614 Accuracy 0.4628\nTime taken for 1 epoch: 203.55441188812256 secs\n\nEpoch 29 Batch 0 Loss 1.4357 Accuracy 0.4628\nEpoch 29 Batch 200 Loss 1.3331 Accuracy 0.4661\nEpoch 29 Batch 400 Loss 1.3235 Accuracy 0.4693\nEpoch 29 Loss 1.3216 Accuracy 0.4709\nTime taken for 1 epoch: 203.46856117248535 secs\n\nEpoch 30 Batch 0 Loss 1.2766 Accuracy 0.4709\nEpoch 30 Batch 200 Loss 1.2936 Accuracy 0.4740\nEpoch 30 Batch 400 Loss 1.2848 Accuracy 0.4772\nEpoch 30 Loss 1.2836 Accuracy 0.4787\nTime taken for 1 epoch: 203.53029918670654 secs\n\nEpoch 31 Batch 0 Loss 1.2252 Accuracy 0.4787\nEpoch 31 Batch 200 Loss 1.2576 Accuracy 0.4817\nEpoch 31 Batch 400 Loss 1.2517 Accuracy 0.4847\nEpoch 31 Loss 1.2504 Accuracy 0.4861\nTime taken for 1 epoch: 204.13799238204956 secs\n\nEpoch 32 Batch 0 Loss 1.2043 Accuracy 0.4862\nEpoch 32 Batch 200 Loss 1.2271 Accuracy 0.4891\nEpoch 32 Batch 400 Loss 1.2190 Accuracy 0.4919\nEpoch 32 Loss 1.2174 Accuracy 0.4933\nTime taken for 1 epoch: 206.50800323486328 secs\n\nEpoch 33 Batch 0 Loss 1.2624 Accuracy 0.4934\nEpoch 33 Batch 200 Loss 1.1966 Accuracy 0.4962\nEpoch 33 Batch 400 Loss 1.1902 Accuracy 0.4989\nEpoch 33 Loss 1.1888 Accuracy 0.5003\nTime taken for 1 epoch: 206.33999013900757 secs\n\nEpoch 34 Batch 0 Loss 1.1159 Accuracy 0.5003\nEpoch 34 Batch 200 Loss 1.1735 Accuracy 0.5030\nEpoch 34 Batch 400 Loss 1.1652 Accuracy 0.5057\nEpoch 34 Loss 1.1640 Accuracy 0.5070\nTime taken for 1 epoch: 205.84175729751587 secs\n\nEpoch 35 Batch 0 Loss 1.1585 Accuracy 0.5070\nEpoch 35 Batch 200 Loss 1.1445 Accuracy 0.5096\nEpoch 35 Batch 400 Loss 1.1361 Accuracy 0.5122\nEpoch 35 Loss 1.1356 Accuracy 0.5134\nTime taken for 1 epoch: 204.14405965805054 secs\n\nEpoch 36 Batch 0 Loss 1.1293 Accuracy 0.5134\nEpoch 36 Batch 200 Loss 1.1146 Accuracy 0.5160\nEpoch 36 Batch 400 Loss 1.1112 Accuracy 0.5184\nEpoch 36 Loss 1.1109 Accuracy 0.5196\nTime taken for 1 epoch: 204.50203442573547 secs\n\nEpoch 37 Batch 0 Loss 1.1539 Accuracy 0.5197\nEpoch 37 Batch 200 Loss 1.0941 Accuracy 0.5221\nEpoch 37 Batch 400 Loss 1.0899 Accuracy 0.5245\nEpoch 37 Loss 1.0890 Accuracy 0.5257\nTime taken for 1 epoch: 203.98152470588684 secs\n\nEpoch 38 Batch 0 Loss 1.0808 Accuracy 0.5257\nEpoch 38 Batch 200 Loss 1.0709 Accuracy 0.5280\nEpoch 38 Batch 400 Loss 1.0675 Accuracy 0.5304\nEpoch 38 Loss 1.0670 Accuracy 0.5315\nTime taken for 1 epoch: 204.05653834342957 secs\n\nEpoch 39 Batch 0 Loss 1.0967 Accuracy 0.5315\nEpoch 39 Batch 200 Loss 1.0521 Accuracy 0.5338\nEpoch 39 Batch 400 Loss 1.0479 Accuracy 0.5360\nEpoch 39 Loss 1.0470 Accuracy 0.5371\nTime taken for 1 epoch: 204.24601364135742 secs\n\nEpoch 40 Batch 0 Loss 1.0263 Accuracy 0.5371\nEpoch 40 Batch 200 Loss 1.0334 Accuracy 0.5393\nEpoch 40 Batch 400 Loss 1.0268 Accuracy 0.5415\nEpoch 40 Loss 1.0263 Accuracy 0.5426\nTime taken for 1 epoch: 204.04088759422302 secs\n\nEpoch 41 Batch 0 Loss 1.0890 Accuracy 0.5426\nEpoch 41 Batch 200 Loss 1.0131 Accuracy 0.5447\nEpoch 41 Batch 400 Loss 1.0095 Accuracy 0.5468\nEpoch 41 Loss 1.0092 Accuracy 0.5478\nTime taken for 1 epoch: 203.99930930137634 secs\n\nEpoch 42 Batch 0 Loss 0.9648 Accuracy 0.5479\nEpoch 42 Batch 200 Loss 0.9948 Accuracy 0.5499\nEpoch 42 Batch 400 Loss 0.9896 Accuracy 0.5520\nEpoch 42 Loss 0.9894 Accuracy 0.5530\nTime taken for 1 epoch: 203.8211772441864 secs\n\nEpoch 43 Batch 0 Loss 1.0374 Accuracy 0.5530\nEpoch 43 Batch 200 Loss 0.9744 Accuracy 0.5550\nEpoch 43 Batch 400 Loss 0.9714 Accuracy 0.5570\nEpoch 43 Loss 0.9712 Accuracy 0.5580\nTime taken for 1 epoch: 204.01033687591553 secs\n\nEpoch 44 Batch 0 Loss 0.8642 Accuracy 0.5580\nEpoch 44 Batch 200 Loss 0.9587 Accuracy 0.5599\nEpoch 44 Batch 400 Loss 0.9557 Accuracy 0.5618\nEpoch 44 Loss 0.9552 Accuracy 0.5628\nTime taken for 1 epoch: 204.29428029060364 secs\n\nEpoch 45 Batch 0 Loss 0.9027 Accuracy 0.5628\nEpoch 45 Batch 200 Loss 0.9416 Accuracy 0.5647\nEpoch 45 Batch 400 Loss 0.9392 Accuracy 0.5666\nEpoch 45 Loss 0.9396 Accuracy 0.5675\nTime taken for 1 epoch: 204.15486979484558 secs\n\nEpoch 46 Batch 0 Loss 0.9669 Accuracy 0.5675\nEpoch 46 Batch 200 Loss 0.9237 Accuracy 0.5693\nEpoch 46 Batch 400 Loss 0.9216 Accuracy 0.5712\nEpoch 46 Loss 0.9213 Accuracy 0.5720\nTime taken for 1 epoch: 204.2621488571167 secs\n\nEpoch 47 Batch 0 Loss 0.8318 Accuracy 0.5721\nEpoch 47 Batch 200 Loss 0.9112 Accuracy 0.5738\nEpoch 47 Batch 400 Loss 0.9089 Accuracy 0.5756\nEpoch 47 Loss 0.9090 Accuracy 0.5765\nTime taken for 1 epoch: 204.43257570266724 secs\n\nEpoch 48 Batch 0 Loss 0.9739 Accuracy 0.5765\nEpoch 48 Batch 200 Loss 0.8974 Accuracy 0.5782\nEpoch 48 Batch 400 Loss 0.8932 Accuracy 0.5799\nEpoch 48 Loss 0.8940 Accuracy 0.5808\nTime taken for 1 epoch: 204.03792572021484 secs\n\nEpoch 49 Batch 0 Loss 0.9436 Accuracy 0.5808\nEpoch 49 Batch 200 Loss 0.8801 Accuracy 0.5825\nEpoch 49 Batch 400 Loss 0.8796 Accuracy 0.5842\nEpoch 49 Loss 0.8792 Accuracy 0.5850\nTime taken for 1 epoch: 204.1065719127655 secs\n\nEpoch 50 Batch 0 Loss 0.9229 Accuracy 0.5850\nEpoch 50 Batch 200 Loss 0.8656 Accuracy 0.5866\nEpoch 50 Batch 400 Loss 0.8631 Accuracy 0.5883\nEpoch 50 Loss 0.8630 Accuracy 0.5891\nTime taken for 1 epoch: 203.88586711883545 secs\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#Lets make predictions\n\ndef evaluate(text):\n    text = eng_tokenizer.texts_to_sequences([text])\n    text = tf.keras.preprocessing.sequence.pad_sequences(text, maxlen=ENCODER_LEN, \n                                                                   padding='post', truncating='post')\n\n    encoder_input = tf.expand_dims(text[0], 0)\n\n    decoder_input = [hind_tokenizer.word_index['<sos>']]\n    output = tf.expand_dims(decoder_input, 0)\n    \n    for i in range(DECODER_LEN):\n        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n\n        predictions, attention_weights = transformer(\n            encoder_input, \n            output,\n            False,\n            enc_padding_mask,\n            combined_mask,\n            dec_padding_mask\n        )\n\n        predictions = predictions[: ,-1:, :]\n        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n\n        if predicted_id == hind_tokenizer.word_index['<eos>']:\n            return tf.squeeze(output, axis=0), attention_weights\n\n        output = tf.concat([output, predicted_id], axis=-1)\n\n    return tf.squeeze(output, axis=0), attention_weights","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:08:53.728192Z","iopub.execute_input":"2022-11-24T21:08:53.730115Z","iopub.status.idle":"2022-11-24T21:08:53.741336Z","shell.execute_reply.started":"2022-11-24T21:08:53.730078Z","shell.execute_reply":"2022-11-24T21:08:53.740441Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def translate(eng_text):\n    hind_text = evaluate(text=eng_text)[0].numpy()\n    hind_text = np.expand_dims(hind_text[1:], 0)  \n    return hind_tokenizer.sequences_to_texts(hind_text)[0]","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:08:53.745101Z","iopub.execute_input":"2022-11-24T21:08:53.747313Z","iopub.status.idle":"2022-11-24T21:08:53.767302Z","shell.execute_reply.started":"2022-11-24T21:08:53.747278Z","shell.execute_reply":"2022-11-24T21:08:53.766615Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Predictions\n\nBelow are 2 translation predictions, the first one is on a sentence that was in the dataset and the other one that I wrote \nby myself.","metadata":{}},{"cell_type":"code","source":"translate(\"I am a student and I like to study very much.\")","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:08:53.773242Z","iopub.execute_input":"2022-11-24T21:08:53.775278Z","iopub.status.idle":"2022-11-24T21:08:55.464268Z","shell.execute_reply.started":"2022-11-24T21:08:53.775244Z","shell.execute_reply":"2022-11-24T21:08:55.463446Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"'मैं एक छात्र हूं और मैं बहुत ही पढ़ने करना चाहता हूं'"},"metadata":{}}]},{"cell_type":"code","source":"translate(\"I am very happy about my family\")","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:08:55.465455Z","iopub.execute_input":"2022-11-24T21:08:55.465810Z","iopub.status.idle":"2022-11-24T21:08:56.835643Z","shell.execute_reply.started":"2022-11-24T21:08:55.465775Z","shell.execute_reply":"2022-11-24T21:08:56.834954Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"'मैं अपने परिवार के बारे मेरे परिवार के बारे में बहुत बहुत खुश हूँ'"},"metadata":{}}]},{"cell_type":"code","source":"translate(\"I want to play cricket\")","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:08:56.836962Z","iopub.execute_input":"2022-11-24T21:08:56.837208Z","iopub.status.idle":"2022-11-24T21:08:58.126997Z","shell.execute_reply.started":"2022-11-24T21:08:56.837176Z","shell.execute_reply":"2022-11-24T21:08:58.126316Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"'मैं क्रिकेट खेल खेल खेल रहा हूँ और मैं क्रिकेट खेल रहा हूँ'"},"metadata":{}}]},{"cell_type":"code","source":"transformer.save_weights('model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:08:58.128296Z","iopub.execute_input":"2022-11-24T21:08:58.128587Z","iopub.status.idle":"2022-11-24T21:08:58.361106Z","shell.execute_reply.started":"2022-11-24T21:08:58.128552Z","shell.execute_reply":"2022-11-24T21:08:58.360327Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}